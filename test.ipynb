{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete later\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # reads .env file in current directory\n",
    "\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "\n",
    "from utils_r2 import (\n",
    "    upload_to_r2,\n",
    "    ensure_folder_exists,\n",
    "    clean_old_backups_r2,\n",
    "    list_r2_files,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "SAVE_DIR = BASE_DIR / \"deposit_rate\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BUCKET = os.getenv(\"R2_BUCKET\")\n",
    "PREFIX_MAIN = \"cafef_data/\"  # master lives here\n",
    "PREFIX_BACKUP_MASTER = \"cafef_data/cafef_data_backup/\"  # keep last 2 master files\n",
    "PREFIX_BACKUP_INDIV = \"cafef_data/cafef_data_backup/deposit_rate/\"  # all daily individual files\n",
    "\n",
    "SEARCH_URL_TEMPLATE = (\n",
    "    \"https://vietnamnet.vn/tim-kiem-p{page}?q=Lãi suất ngân hàng hôm nay&od=2&bydaterang=all&newstype=all\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SMALL HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def yyyymmdd_from_ddmmyyyy(s):\n",
    "    # \"25/10/2025\" -> \"2025-10-25\"\n",
    "    return datetime.strptime(s, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def ddmmyy_from_yyyy_mm_dd(s):\n",
    "    # \"2025-10-25\" -> \"251025\"\n",
    "    return datetime.strptime(s, \"%Y-%m-%d\").strftime(\"%d%m%y\")\n",
    "\n",
    "def extract_article_date_from_title(title_text):\n",
    "    \"\"\"\n",
    "    Tries to find a dd/mm/yyyy in the article title and return 'YYYY-MM-DD'\n",
    "    If not found: return None\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})\", title_text)\n",
    "    if not m:\n",
    "        return None\n",
    "    return yyyymmdd_from_ddmmyyyy(m.group(1))\n",
    "\n",
    "def safe_float(cell_text):\n",
    "    \"\"\"\n",
    "    Turn cell text like '4,65' or '4.7%' into float 4.65\n",
    "    Return None if cannot parse or >100 (nonsense)\n",
    "    \"\"\"\n",
    "    txt = cell_text.strip().replace(\",\", \".\").replace(\"%\", \"\")\n",
    "    try:\n",
    "        val = float(txt)\n",
    "        if val > 100:\n",
    "            return None\n",
    "        return val\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def safe_goto(page, url, max_retries=3, timeout=60000):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            await page.goto(url, timeout=timeout)\n",
    "            await page.wait_for_load_state(\"domcontentloaded\", timeout=15000)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Navigation attempt {attempt}/{max_retries} failed: {e}\")\n",
    "            await asyncio.sleep(2)\n",
    "    return False\n",
    "\n",
    "def normalize_rate_table(html, fallback_date):\n",
    "    \"\"\"\n",
    "    Parse a vietnamnet interest-rate table HTML → pandas.DataFrame\n",
    "    Columns:\n",
    "      date, bank, no_term, 1m, 3m, 6m, 12m, ...\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # detect date in page body; fallback to article date string if not found\n",
    "    match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})\", soup.get_text(\" \", strip=True))\n",
    "    table_date = (\n",
    "        yyyymmdd_from_ddmmyyyy(match.group(1))\n",
    "        if match\n",
    "        else fallback_date\n",
    "    )\n",
    "\n",
    "    # find \"main\" table\n",
    "    target_table = None\n",
    "    for tbl in soup.find_all(\"table\"):\n",
    "        text = tbl.get_text(\" \", strip=True).lower()\n",
    "        # crude heuristic: banking rates table tends to include these keywords\n",
    "        if all(k in text for k in [\"ngân\", \"tháng\", \"lãi suất\"]):\n",
    "            target_table = tbl\n",
    "            break\n",
    "\n",
    "    if target_table is None:\n",
    "        print(\"⚠️ No valid table found in this article.\")\n",
    "        return None\n",
    "\n",
    "    rows = target_table.find_all(\"tr\")\n",
    "    if len(rows) < 3:\n",
    "        print(\"⚠️ Table is too short (<3 rows).\")\n",
    "        return None\n",
    "\n",
    "    # header row typically at rows[1]\n",
    "    header_cells = rows[1].find_all([\"td\", \"th\"])\n",
    "    headers = [\"bank\"] + [\n",
    "        re.sub(r\"\\s+\", \"_\", c.get_text(strip=True).lower()) for c in header_cells[1:]\n",
    "    ]\n",
    "\n",
    "    data_rows = []\n",
    "    for row in rows[2:]:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "\n",
    "        bank_name = cells[0].get_text(strip=True).upper()\n",
    "        rate_values = []\n",
    "        for c in cells[1:]:\n",
    "            rate_values.append(safe_float(c.get_text(strip=True)))\n",
    "\n",
    "        rec = {\"date\": table_date, \"bank\": bank_name}\n",
    "        for i, h in enumerate(headers[1:]):\n",
    "            rec[h] = rate_values[i] if i < len(rate_values) else None\n",
    "        data_rows.append(rec)\n",
    "\n",
    "    df = pd.DataFrame(data_rows)\n",
    "\n",
    "    # rename columns like \"3_tháng\" → \"3m\", \"không_kỳ_hạn\" → \"no_term\"\n",
    "    rename_map = {}\n",
    "    for col in df.columns:\n",
    "        cclean = col.lower().strip()\n",
    "        # map tenor columns\n",
    "        if \"tháng\" in cclean:\n",
    "            m = re.search(r\"(\\d+)\", cclean)\n",
    "            if m:\n",
    "                rename_map[col] = f\"{m.group(1)}m\"\n",
    "        # map no-term column\n",
    "        if \"không\" in cclean or \"kỳ_hạn\" in cclean:\n",
    "            rename_map[col] = \"no_term\"\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # final numeric cleanup: ensure all tenor cols are numeric and <=100\n",
    "    tenor_cols = [c for c in df.columns if re.match(r\"^\\d+m$\", c) or c == \"no_term\"]\n",
    "    for c in tenor_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.loc[df[c] > 100, c] = None\n",
    "\n",
    "    # make sure we don't get NaNs to object dtype surprises later\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_existing_parquet_dates_local():\n",
    "    \"\"\"\n",
    "    Look in SAVE_DIR for deposit_rate_individual_<DDMMYY>.parquet.\n",
    "    Return set of \"YYYY-MM-DD\" we've already saved.\n",
    "    \"\"\"\n",
    "    existing = set()\n",
    "    for p in SAVE_DIR.glob(\"deposit_rate_individual_*.parquet\"):\n",
    "        m = re.search(r\"deposit_rate_individual_(\\d{6})\\.parquet$\", p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        ddmmyy = m.group(1)  # e.g. \"251025\"\n",
    "        # turn DDMMYY -> YYYY-MM-DD\n",
    "        dt = datetime.strptime(ddmmyy, \"%d%m%y\").strftime(\"%Y-%m-%d\")\n",
    "        existing.add(dt)\n",
    "    return existing\n",
    "\n",
    "\n",
    "def get_existing_parquet_dates_r2():\n",
    "    \"\"\"\n",
    "    Check R2 under PREFIX_BACKUP_INDIV for deposit_rate_individual_<DDMMYY>.parquet\n",
    "    so we don't re-scrape days we already uploaded.\n",
    "    \"\"\"\n",
    "    keys = list_r2_files(BUCKET, PREFIX_BACKUP_INDIV)\n",
    "    existing = set()\n",
    "    for key in keys:\n",
    "        m = re.search(r\"deposit_rate_individual_(\\d{6})\\.parquet$\", key)\n",
    "        if not m:\n",
    "            continue\n",
    "        ddmmyy = m.group(1)\n",
    "        dt = datetime.strptime(ddmmyy, \"%d%m%y\").strftime(\"%Y-%m-%d\")\n",
    "        existing.add(dt)\n",
    "    return existing\n",
    "\n",
    "\n",
    "async def scrape_vietnamnet_interest_range(start_date=None, end_date=None, headless=True):\n",
    "    \"\"\"\n",
    "    Scrape interest-rate articles in a date range.\n",
    "    - If start_date/end_date not given: just scrape today's latest.\n",
    "    - If given: walk pages, collect all dates within [start_date, end_date].\n",
    "      Stop early if we go past start_date or reach last available page.\n",
    "    For each day not already captured in parquet (local or R2), save\n",
    "    deposit_rate_individual_<DDMMYY>.parquet.\n",
    "    Returns list of all local parquet paths we produced/confirmed for that run.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- parse range boundaries ---\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\").date() if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\").date() if end_date else None\n",
    "\n",
    "    # --- collect existing files (local + R2) ---\n",
    "    already_have = get_existing_parquet_dates_local() | get_existing_parquet_dates_r2()\n",
    "    scraped_parquet_paths = []\n",
    "\n",
    "    async with async_playwright() as pw:\n",
    "        browser = await pw.chromium.launch(headless=headless)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        page_num = 1\n",
    "        keep_scraping = True\n",
    "\n",
    "        while keep_scraping:\n",
    "            search_url = SEARCH_URL_TEMPLATE.format(page=page_num)\n",
    "            print(f\"\\n🔍 Scanning search page {page_num} → {search_url}\")\n",
    "            ok = await safe_goto(page, search_url)\n",
    "            if not ok:\n",
    "                print(\"⚠️ Could not load search page, stopping.\")\n",
    "                break\n",
    "\n",
    "            # --- wait for articles to appear ---\n",
    "            try:\n",
    "                await page.wait_for_selector(\"h3.vnn-title a\", timeout=15000)\n",
    "            except PlaywrightTimeoutError:\n",
    "                print(\"🧭 Reached last page — no search results found.\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "\n",
    "            # --- extract all article links on this page ---\n",
    "            articles = []\n",
    "            for a in soup.select(\"h3.vnn-title a[href]\"):\n",
    "                title = a.get_text(strip=True)\n",
    "                href = a[\"href\"]\n",
    "                full_link = href if href.startswith(\"http\") else f\"https://vietnamnet.vn{href}\"\n",
    "                art_date_str = extract_article_date_from_title(title)\n",
    "                if not art_date_str:\n",
    "                    continue\n",
    "                art_dt = datetime.strptime(art_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "                # --- range filtering ---\n",
    "                if end_dt and art_dt > end_dt:\n",
    "                    continue\n",
    "                if start_dt and art_dt < start_dt:\n",
    "                    print(f\"⏹️ Reached older than {start_date} (found {art_date_str}) → stopping.\")\n",
    "                    keep_scraping = False\n",
    "                    break\n",
    "\n",
    "                articles.append((art_dt, art_date_str, full_link))\n",
    "\n",
    "            # --- stop if no more results ---\n",
    "            if not articles:\n",
    "                print(f\"🧭 Page {page_num} has no valid interest-rate articles → end of archive.\")\n",
    "                break\n",
    "\n",
    "            # --- no-date mode: only scrape today's newest ---\n",
    "            if start_dt is None and end_dt is None:\n",
    "                articles.sort(reverse=True)\n",
    "                art_dt, art_date_str, link = articles[0]\n",
    "                if art_date_str in already_have:\n",
    "                    print(f\"✔️ Already have {art_date_str}, skipping scrape.\")\n",
    "                else:\n",
    "                    path = await scrape_single_article_to_parquet(page, link, art_date_str)\n",
    "                    if path:\n",
    "                        scraped_parquet_paths.append(path)\n",
    "                break\n",
    "\n",
    "            # --- historical range mode ---\n",
    "            for art_dt, art_date_str, link in articles:\n",
    "                if art_date_str in already_have:\n",
    "                    print(f\"✔️ {art_date_str} already exists, skip.\")\n",
    "                    continue\n",
    "                path = await scrape_single_article_to_parquet(page, link, art_date_str)\n",
    "                if path:\n",
    "                    scraped_parquet_paths.append(path)\n",
    "\n",
    "            # --- move to next page ---\n",
    "            page_num += 1\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"\\n✅ Finished scanning. Total new daily files: {len(scraped_parquet_paths)}\")\n",
    "    return scraped_parquet_paths\n",
    "\n",
    "\n",
    "async def scrape_single_article_to_parquet(page, article_url, date_str):\n",
    "    \"\"\"\n",
    "    Go to one vietnamnet article URL, parse table, save parquet for that date.\n",
    "    date_str is 'YYYY-MM-DD'.\n",
    "    Returns the parquet path if saved, else None.\n",
    "    \"\"\"\n",
    "    print(f\"📄 Fetching article {article_url} (date {date_str})\")\n",
    "\n",
    "    ok = await safe_goto(page, article_url)\n",
    "    if not ok:\n",
    "        print(f\"⚠️ Failed nav to {article_url}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table\", timeout=20000)\n",
    "    except PlaywrightTimeoutError:\n",
    "        print(\"⚠️ No table found in article.\")\n",
    "        return None\n",
    "\n",
    "    html = await page.content()\n",
    "    df_day = normalize_rate_table(html, date_str)\n",
    "    if df_day is None or df_day.empty:\n",
    "        print(\"⚠️ Parsed table is empty.\")\n",
    "        return None\n",
    "\n",
    "    # save parquet as deposit_rate_individual_<DDMMYY>.parquet\n",
    "    ddmmyy = ddmmyy_from_yyyy_mm_dd(date_str)\n",
    "    parquet_path = SAVE_DIR / f\"deposit_rate_individual_{ddmmyy}.parquet\"\n",
    "    df_day.to_parquet(parquet_path, index=False, compression=\"gzip\")\n",
    "    print(f\"💾 Saved {parquet_path} ({len(df_day)} rows)\")\n",
    "\n",
    "    return parquet_path\n",
    "\n",
    "\n",
    "def build_master_parquet(master_path):\n",
    "    \"\"\"\n",
    "    Read every deposit_rate_individual_*.parquet in SAVE_DIR,\n",
    "    concat, drop duplicates (date+bank), sort by date+bank,\n",
    "    and save as master parquet.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for p in SAVE_DIR.glob(\"deposit_rate_individual_*.parquet\"):\n",
    "        try:\n",
    "            dfp = pd.read_parquet(p)\n",
    "            parts.append(dfp)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read {p}: {e}\")\n",
    "\n",
    "    if not parts:\n",
    "        print(\"⚠️ No daily parquet files found to build master.\")\n",
    "        return None\n",
    "\n",
    "    big = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # drop duplicate rows by same (date, bank)\n",
    "    if \"date\" in big.columns and \"bank\" in big.columns:\n",
    "        big = big.drop_duplicates(subset=[\"date\", \"bank\"])\n",
    "\n",
    "    # sort by date then bank for consistency\n",
    "    big = big.sort_values([\"date\", \"bank\"]).reset_index(drop=True)\n",
    "\n",
    "    big.to_parquet(master_path, index=False, compression=\"gzip\")\n",
    "    print(f\"🏦 Master parquet saved → {master_path} ({len(big)} rows)\")\n",
    "    return master_path\n",
    "\n",
    "\n",
    "def sync_to_r2(today_ddmmyy, master_path):\n",
    "    \"\"\"\n",
    "    Uploads:\n",
    "    - master parquet -> cafef_data/deposit_rate_<DDMMYY>.parquet\n",
    "    - master parquet -> cafef_data/cafef_data_backup/deposit_rate_<DDMMYY>.parquet (keep 2 latest)\n",
    "    - individual day parquet(s) -> cafef_data/cafef_data_backup/deposit_rate/*.parquet (append-only)\n",
    "    Also trims backup masters to keep=2.\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure folders exist in R2\n",
    "    ensure_folder_exists(BUCKET, PREFIX_MAIN)\n",
    "    ensure_folder_exists(BUCKET, PREFIX_BACKUP_MASTER)\n",
    "    ensure_folder_exists(BUCKET, PREFIX_BACKUP_INDIV)\n",
    "\n",
    "    # 1) upload master to main\n",
    "    main_key = f\"{PREFIX_MAIN}deposit_rate_{today_ddmmyy}.parquet\"\n",
    "    upload_to_r2(master_path, BUCKET, main_key)\n",
    "    print(f\"☁️ Uploaded master → {main_key}\")\n",
    "\n",
    "    # 2) upload master to backup_master, then prune\n",
    "    backup_master_key = f\"{PREFIX_BACKUP_MASTER}deposit_rate_{today_ddmmyy}.parquet\"\n",
    "    upload_to_r2(master_path, BUCKET, backup_master_key)\n",
    "    clean_old_backups_r2(BUCKET, PREFIX_BACKUP_MASTER, keep=2)\n",
    "\n",
    "    # 3) upload individual daily files to backup/deposit_rate\n",
    "    for p in SAVE_DIR.glob(\"deposit_rate_individual_*.parquet\"):\n",
    "        upload_to_r2(\n",
    "            p,\n",
    "            BUCKET,\n",
    "            f\"{PREFIX_BACKUP_INDIV}{p.name}\"\n",
    "        )\n",
    "    print(\"📤 Synced individual daily files as well.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PUBLIC ENTRYPOINT\n",
    "# ============================================================\n",
    "\n",
    "def run_deposit_rate_scraper(start_date=None, end_date=None, headless=True):\n",
    "    \"\"\"\n",
    "    High-level flow:\n",
    "    1. scrape_vietnamnet_interest_range(...) to create missing daily parquet(s)\n",
    "    2. build/update master parquet from all local dailies\n",
    "    3. upload master + daily parquets to R2 (and rotate backups)\n",
    "    \"\"\"\n",
    "\n",
    "    # step 1: scrape + save any missing daily parquet(s)\n",
    "    scraped_paths = asyncio.run(\n",
    "        scrape_vietnamnet_interest_range(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            headless=headless,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if scraped_paths:\n",
    "        print(f\"🆕 Got {len(scraped_paths)} new daily parquet file(s).\")\n",
    "    else:\n",
    "        print(\"ℹ️ No new parquet scraped (all dates already exist).\")\n",
    "\n",
    "    # define today's tag (DDMMYY) for naming master\n",
    "    # if you passed a range, we'll use end_date as \"today\" tag so you know which run it came from\n",
    "    tag_date = end_date if end_date else datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    today_ddmmyy = datetime.strptime(tag_date, \"%Y-%m-%d\").strftime(\"%d%m%y\")\n",
    "\n",
    "    master_path = SAVE_DIR / f\"deposit_rate_{today_ddmmyy}.parquet\"\n",
    "\n",
    "    # step 2: build master parquet from all daily files\n",
    "    master_path = build_master_parquet(master_path)\n",
    "    if master_path is None:\n",
    "        print(\"❌ No master created, skipping R2 sync.\")\n",
    "        return\n",
    "\n",
    "    # step 3: push to R2\n",
    "    # sync_to_r2(today_ddmmyy, master_path)\n",
    "    # print(\"✅ Deposit rate sync completed.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL MANUAL RUN EXAMPLES\n",
    "# ============================================================\n",
    "# Example 1: only today (latest article only)\n",
    "# run_deposit_rate_scraper()\n",
    "\n",
    "# Example 2: historical range (scrape between 2025-09-01 and 2025-10-25)\n",
    "# run_deposit_rate_scraper(start_date=\"2025-09-01\", end_date=\"2025-10-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Page 1\n",
      "📄 Fetching https://vietnamnet.vn/lai-suat-ngan-hang-hom-nay-29-9-2025-tang-lai-suat-ky-han-1-11-thang-2447092.html\n",
      "💾 Saved Parquet → /Users/leeboo/Library/CloudStorage/OneDrive-Personal/Personal data/Study/Trading projects/Webscrapping data/market_data/cafef_data_github/deposit_rate/deposit_rate_individual_271025.parquet (34 rows)\n",
      "📁 Created folder: cafef_data/cafef_data_backup/deposit_rate/\n",
      "☁️ Uploaded → s3://broker-data/cafef_data/deposit_rate_271025.parquet\n",
      "☁️ Uploaded master → cafef_data/deposit_rate_271025.parquet\n",
      "☁️ Uploaded → s3://broker-data/cafef_data/cafef_data_backup/deposit_rate_271025.parquet\n",
      "☁️ Uploaded → s3://broker-data/cafef_data/cafef_data_backup/deposit_rate/deposit_rate_individual_271025.parquet\n",
      "✅ Uploaded backups for 271025\n"
     ]
    }
   ],
   "source": [
    "run_deposit_rate_scraper(start_date=\"2000-09-01\", end_date=\"2025-10-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc8cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leeboo_st445",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
