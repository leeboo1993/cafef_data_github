# ======================================================
# gold_price_scraper.py  â€”  Production version (local + GitHub Actions)
# ======================================================

import os, re
import requests
import pandas as pd
from datetime import datetime
from pathlib import Path

# Try dotenv for local runs; safe no-op in GitHub Actions
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

from utils_r2 import (
    upload_to_r2,
    clean_old_backups_r2,
    list_r2_files,
)

# ======================================================
# CONFIGURATION
# ======================================================
HEADERS = {"User-Agent": "Mozilla/5.0"}
SAVE_DIR = Path.cwd() / "gold_price"
SAVE_DIR.mkdir(parents=True, exist_ok=True)

BUCKET = os.getenv("R2_BUCKET")

# Folder structure on R2
PREFIX_MAIN = "cafef_data/"
PREFIX_BACKUP = "cafef_data/cafef_data_backup/gold_price/"

print("ğŸª™ Fetching SJC gold data (incremental update, no spreads)...")

# ======================================================
# 1ï¸âƒ£ FETCH GOLD DATA FROM CAFEF
# ======================================================
def fetch_gold_data():
    urls = {
        "bar": "https://cafef.vn/du-lieu/Ajax/ajaxgoldpricehistory.ashx?index=all",
        "ring": "https://cafef.vn/du-lieu/Ajax/AjaxGoldPriceRing.ashx?time=all&zone=11"
    }

    def fetch(url):
        r = requests.get(url, headers=HEADERS, timeout=30)
        r.raise_for_status()
        return r.json().get("Data", {}).get("goldPriceWorldHistories", [])

    # --- Download JSON ---
    df_bar = pd.DataFrame(fetch(urls["bar"]))
    df_ring = pd.DataFrame(fetch(urls["ring"]))

    # --- Identify correct timestamp field ---
    date_col_bar = "createdAt" if "createdAt" in df_bar.columns else "lastUpdated"
    date_col_ring = "createdAt" if "createdAt" in df_ring.columns else "lastUpdated"

    # --- Parse to datetime robustly ---
    df_bar["date"] = pd.to_datetime(df_bar[date_col_bar], errors="coerce", format="mixed", utc=True)
    df_ring["date"] = pd.to_datetime(df_ring[date_col_ring], errors="coerce", format="mixed", utc=True)

    # --- Drop invalid dates ---
    df_bar = df_bar.dropna(subset=["date"])
    df_ring = df_ring.dropna(subset=["date"])

    # --- Rename columns for clarity ---
    df_bar = df_bar.rename(columns={"buyPrice": "bar_buy", "sellPrice": "bar_sell"})
    df_ring = df_ring.rename(columns={"buyPrice": "ring_buy", "sellPrice": "ring_sell"})

    # --- Merge ---
    df = pd.merge(
        df_bar[["date", "bar_buy", "bar_sell"]],
        df_ring[["date", "ring_buy", "ring_sell"]],
        on="date",
        how="outer"
    )

    # --- Normalize time zone and sort ---
    df["date"] = df["date"].dt.tz_localize(None)
    df = df.sort_values("date").reset_index(drop=True)

    # --- Drop rows with no valid prices ---
    df = df.dropna(subset=["bar_buy", "bar_sell", "ring_buy", "ring_sell"], how="all")

    print(f"âœ… Retrieved {len(df)} gold records.")
    return df


# ======================================================
# 2ï¸âƒ£ INCREMENTAL UPDATE LOGIC
# ======================================================
def incremental_update(new_df, local_path):
    """Merge new data with existing local parquet (if present)."""
    if os.path.exists(local_path):
        old_df = pd.read_parquet(local_path)
        before = len(old_df)
        combined = pd.concat([old_df, new_df], ignore_index=True)
        combined = combined.drop_duplicates(subset=["date"]).sort_values("date").reset_index(drop=True)
        after = len(combined)
        print(f"ğŸ“ˆ Incremental update: {after - before} new rows added.")
    else:
        combined = new_df
        print(f"ğŸ†• First-time load: {len(new_df)} rows saved.")
    return combined


# ======================================================
# 3ï¸âƒ£ CHECK IF UPDATE NEEDED (skip logic)
# ======================================================
def latest_r2_date(bucket, prefix):
    """Return latest date from file name on R2 (gold_price_DDMMYY.parquet)."""
    files = list_r2_files(bucket, prefix)
    dates = []
    for f in files:
        m = re.search(r"gold_price_(\d{6})\.parquet$", f)
        if m:
            try:
                d = datetime.strptime(m.group(1), "%d%m%y")
                dates.append(d)
            except:
                pass
    return max(dates) if dates else None


# ======================================================
# 4ï¸âƒ£ MAIN SCRIPT
# ======================================================
def update_gold_prices():
    today = datetime.now()
    today_str = today.strftime("%d%m%y")
    parquet_path = SAVE_DIR / f"gold_price_{today_str}.parquet"

    # --- Skip if already up-to-date on R2 ---
    latest_remote = latest_r2_date(BUCKET, PREFIX_MAIN)
    if latest_remote and latest_remote.date() >= today.date():
        print(f"âœ… Already up-to-date ({latest_remote.strftime('%d/%m/%Y')}) â†’ skip download.")
        return

    # --- ğŸ§¹ Clean local/GitHub cache before rebuild ---
    for f in SAVE_DIR.glob("gold_price_*.parquet"):
        try:
            os.remove(f)
            print(f"ğŸ§¹ Deleted old local cache file: {f}")
        except Exception as e:
            print(f"âš ï¸ Could not delete {f}: {e}")

    # --- Fetch new data ---
    df = fetch_gold_data()

    # --- Merge with local if exists ---
    combined = incremental_update(df, parquet_path)

    # --- Save local parquet ---
    combined.to_parquet(parquet_path, index=False, compression="gzip")
    print(f"ğŸ’¾ Saved Parquet â†’ {parquet_path} ({len(combined)} rows)")

    # --- Upload new file ---
    upload_to_r2(parquet_path, BUCKET, f"{PREFIX_MAIN}{parquet_path.name}")

    # --- Keep 2 backups on R2 ---
    clean_old_backups_r2(BUCKET, PREFIX_BACKUP, keep=2)

    print("â˜ï¸ Uploaded new gold data and maintained backup rotation.")

    # --- ğŸ§¹ Final local cleanup ---
    try:
        for f in SAVE_DIR.glob("*.parquet"):
            os.remove(f)
            print(f"ğŸ§¹ Deleted local file: {f}")
        if not any(SAVE_DIR.iterdir()):
            SAVE_DIR.rmdir()
            print(f"ğŸ—‘ï¸ Removed empty folder: {SAVE_DIR}")
    except Exception as e:
        print(f"âš ï¸ Cleanup error: {e}")


# ======================================================
# 5ï¸âƒ£ ENTRY POINT
# ======================================================
if __name__ == "__main__":
    update_gold_prices()
    print("âœ… Gold price update completed successfully.")